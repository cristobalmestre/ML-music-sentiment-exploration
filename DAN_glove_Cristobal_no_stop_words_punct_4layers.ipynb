{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYyXv7ZbVr-P",
        "outputId": "7943d149-2ba1-41f5-9338-b4db646edaf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun May 12 04:05:56 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   57C    P8              11W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-l_xeCKVwup",
        "outputId": "9bb3b8e6-5b2b-4c44-cce9-968529c2b5c9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 54.8 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langdetect"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqrK-wMwVy2e",
        "outputId": "663d4bc2-4030-48d7-cb26-acf012ae1938"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.10/dist-packages (1.0.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from langdetect import detect\n",
        "import re"
      ],
      "metadata": {
        "id": "gmL4KYQJjQv2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('balanced.csv') #upload csv manually!!\n",
        "\n",
        "def clean(paragraph, words_to_remove):\n",
        "    paragraph = paragraph.lower()\n",
        "    paragraph = re.sub(r'\\d+', '', paragraph)\n",
        "    for word in words_to_remove:\n",
        "        paragraph = paragraph.replace(word, \"\")\n",
        "    paragraph = paragraph.replace(\". \", \".\")\n",
        "    return paragraph\n",
        "\n",
        "words_to_remove = [\"[\",\"]\",\"-\",\"+\",\"verse\",\"chorus\",\"outro\",\"intro\",\":\"]\n",
        "\n",
        "balanced_data = df.copy()\n",
        "balanced_data['lyrics'] = balanced_data['lyrics'].apply(lambda x: clean(x, words_to_remove))"
      ],
      "metadata": {
        "id": "w0fLN3kmihm1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "Z36AzmExjvBH",
        "outputId": "05a01fdb-9c12-4804-eb51-3172815f313b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       valence_tags  positivity  \\\n",
              "0          3.000000           0   \n",
              "1          3.858696           0   \n",
              "2          7.616667           1   \n",
              "3          6.685000           1   \n",
              "4          3.240000           0   \n",
              "...             ...         ...   \n",
              "16841      7.807083           1   \n",
              "16842      5.998475           1   \n",
              "16843      3.160000           0   \n",
              "16844      2.530000           0   \n",
              "16845      8.470000           1   \n",
              "\n",
              "                                                  lyrics  \n",
              "0       .from the haunts of daily life.where is waged...  \n",
              "1      i was waiting for a mermaid to appear.saying c...  \n",
              "2       .i waited for you winterlong.you seem to be w...  \n",
              "3      the world is giving you the run around.it leav...  \n",
              "4       .you know what they say.that everything in yo...  \n",
              "...                                                  ...  \n",
              "16841  if we received a warning call.would we change ...  \n",
              "16842   .i know so many.places in the world.i follow ...  \n",
              "16843  i bring you death.and steal your breath.i am y...  \n",
              "16844  close your eyes, we're coming down.close your ...  \n",
              "16845  in the backyard.in the living room.residential...  \n",
              "\n",
              "[16846 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a79dd336-6fff-43cb-aec7-fcdf49d187a9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>valence_tags</th>\n",
              "      <th>positivity</th>\n",
              "      <th>lyrics</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>.from the haunts of daily life.where is waged...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3.858696</td>\n",
              "      <td>0</td>\n",
              "      <td>i was waiting for a mermaid to appear.saying c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7.616667</td>\n",
              "      <td>1</td>\n",
              "      <td>.i waited for you winterlong.you seem to be w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6.685000</td>\n",
              "      <td>1</td>\n",
              "      <td>the world is giving you the run around.it leav...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3.240000</td>\n",
              "      <td>0</td>\n",
              "      <td>.you know what they say.that everything in yo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16841</th>\n",
              "      <td>7.807083</td>\n",
              "      <td>1</td>\n",
              "      <td>if we received a warning call.would we change ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16842</th>\n",
              "      <td>5.998475</td>\n",
              "      <td>1</td>\n",
              "      <td>.i know so many.places in the world.i follow ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16843</th>\n",
              "      <td>3.160000</td>\n",
              "      <td>0</td>\n",
              "      <td>i bring you death.and steal your breath.i am y...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16844</th>\n",
              "      <td>2.530000</td>\n",
              "      <td>0</td>\n",
              "      <td>close your eyes, we're coming down.close your ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16845</th>\n",
              "      <td>8.470000</td>\n",
              "      <td>1</td>\n",
              "      <td>in the backyard.in the living room.residential...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>16846 rows × 3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a79dd336-6fff-43cb-aec7-fcdf49d187a9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a79dd336-6fff-43cb-aec7-fcdf49d187a9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a79dd336-6fff-43cb-aec7-fcdf49d187a9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1fc05191-674e-4e81-8860-0ac54050f4a3\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1fc05191-674e-4e81-8860-0ac54050f4a3')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1fc05191-674e-4e81-8860-0ac54050f4a3 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_4ada60e0-1efc-4779-aeb2-543c2a1dbef7\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('balanced_data')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_4ada60e0-1efc-4779-aeb2-543c2a1dbef7 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('balanced_data');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "balanced_data",
              "summary": "{\n  \"name\": \"balanced_data\",\n  \"rows\": 16846,\n  \"fields\": [\n    {\n      \"column\": \"valence_tags\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.6704655510175694,\n        \"min\": 0.235,\n        \"max\": 8.47,\n        \"num_unique_values\": 8657,\n        \"samples\": [\n          5.35625,\n          3.32,\n          2.676960784\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"positivity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lyrics\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 16843,\n        \"samples\": [\n          \" .i'm getting lost in your curls.i'm drawing pictures on your skin.so soft it twirls.i like your looks when you get mean.i know i shouldn't say so but when you claw me like a cat.i'm beaming.i like the way you squeeze my hand.pulling me into another dream.a lucid dream.i'm getting lost in your curls.i'm getting crushed out on the things that only i should see..pre.and not for boys, they're just for me.hurry to talk from far away.i can see you, you curl your fists and you pull your hair.when we're alone i wanna say.let's just stay in.no one's here in our apartment, babe..put on the dress that i like.it makes me so crazy though i can't say why.keep on your stockings for a while.some kind of magic in the way you're lying there.. .i'm getting lost in your curls.i'm getting rushed back on a whim.our breaths get wind.back to the time when we were green.i know we have changed but i still grin.cause i can't wait to see you.back to the time i touched your hair.when i was so scared to look that mean, i think it's weird.i'm getting lost in your curls.i'm getting crushed out on the things that only i should see..pre.they're not for boys, they're just for me.girl, we could talk far away.it's so hard for me only to get the urge to kiss you there.when we're alone i wanna say.let's just stay in.no one's here in our apartment, babe..put on the dress that i like.it makes me so crazy though i can't say why.keep on your stockings for a while.some kind of magic in the way you're lying there.put on the dress that i like.it makes me so crazy though i can't say why.keep on your stockings for a while.some kind of magic in the way you talk about your...blue eyeshadow, it's not exactly blue, no.but you refuse to call it anything but your blue\",\n          \"it's a long time since i saw you last.tell me how you've been.did you ever get to buy that yellow coat?.do the flowers in your window box.still smile when you walk in?.did you ever read the letters that i wrote?.and i've been on the road since christmas.but it don't seem so long.outside of that there isn't much to say.i cut down on my drinkin'.and i wrote another song.i wish you wouldn't look at me that way.remember all the mornings.that we walked around the park.the nights we babysat for billy's kids.and all the times we used to talk.of having one ourselves.i don't remember why we never did.do your neighbors still complain a bit.when the music gets too loud?.does your old cat still sleep up on the bed?.do you still walk around.as if your head was in the clouds?.have you heard a single thing i've said?.and it's a two day drive to new york.i guess i'd better go.did you notice that the weather's gettin' cold?.and it's a long time since i saw you last.and tell me how you've been.did you ever get to buy that yellow coat?.did you ever get to buy that yellow coat?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator, GloVe\n",
        "from torchtext.vocab import vocab\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, f1_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as pl\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "balanced_data = balanced_data[['lyrics', 'positivity']]  # select only the necessary columns\n",
        "\n",
        "# Split dataset into training and validation\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(balanced_data['lyrics'], balanced_data['positivity'], test_size=0.1, random_state=42)\n"
      ],
      "metadata": {
        "id": "DsnBm7olvxRz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5eb283ed-8fe3-429b-fa69-6435b3c1af1c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load English stopwords from nltk\n",
        "stopwords = set(stopwords.words('english'))\n",
        "\n",
        "# Include punctuation in the set of characters to remove\n",
        "stopwords.update(string.punctuation)\n",
        "\n",
        "# Prepare tokenizer and vocabulary\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "# Tokenize (all data, train, and validation)\n",
        "tokenized_data = [tokenizer(txt) for txt in balanced_data['lyrics']]  # tokenize all data\n",
        "train_tokenized = [tokenizer(text) for text in train_texts] # tokenize the data (only train)\n",
        "val_tokenized = [tokenizer(text) for text in val_texts] # tokenize the data (only validation)\n",
        "\n",
        "\n",
        "# Filter out stopwords in the tokenization step\n",
        "filtered_tokenized_data = [[token for token in tokens if token.lower() not in stopwords] for tokens in tokenized_data]\n",
        "filtered_train_tokenized = [[token for token in tokens if token.lower() not in stopwords] for tokens in train_tokenized]\n",
        "filtered_val_tokenized = [[token for token in tokens if token.lower() not in stopwords] for tokens in val_tokenized]\n"
      ],
      "metadata": {
        "id": "x8LIiqBN2ctM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocab(tokenized_data):\n",
        "    counter = Counter()\n",
        "    for text in tokenized_data:\n",
        "        counter.update(text)\n",
        "    #return vocab(counter)  # This initializes the vocab\n",
        "    return vocab(counter, specials=('<unk>', '<pad>'))  # This initializes the vocab\n",
        "\n",
        "# Assuming tokenized_texts is already defined\n",
        "my_vocab = build_vocab(filtered_train_tokenized)\n",
        "my_vocab.set_default_index(my_vocab['<unk>'])"
      ],
      "metadata": {
        "id": "C9VSLQNhwmxj"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert tokens to indices\n",
        "train_indices = [[my_vocab[token] for token in tokens] for tokens in filtered_train_tokenized]\n",
        "val_indices = [[my_vocab[token] for token in tokens] for tokens in filtered_val_tokenized]"
      ],
      "metadata": {
        "id": "Q7IuTqi2pkH0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenized_data\n",
        "# Assuming tokenized_data is a list of lists of tokens\n",
        "for i, tokens in enumerate(filtered_tokenized_data):\n",
        "    print(f\"Sentence {i+1}: {tokens}\")\n",
        "    if i == 4:  # Limit the output to the first 5 sentences\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LH0gEB2wDei",
        "outputId": "1ef90bf7-cd30-49ef-e757-1d7674fe3b0d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence 1: ['haunts', 'daily', 'life', 'waged', 'daily', 'strife', 'common', 'wants', 'common', 'cares', 'cuts', 'human', 'heart', 'tears', 'rise', 'like', 'lions', 'slumber', 'in—', 'greatly', 'unknowable', 'numbers', 'let', 'tyrants', 'pour', 'around', 'apocalyptic', 'sound', 'charge', 'iron', 'wheels', 'crash', 'horses', 'heels', 'rise', 'like', 'lions', 'slumber', 'in—', 'greatly', 'unknowable', 'numbers', 'free', 'blood', 'must', 'ensue', 'many', 'workhouse', 'prison', 'pale', 'corpses', 'newly', 'risen', 'knives', 'drawn', 'let', 'see', 'standing', 'tall', 'say', 'free', 'strong', 'simple', 'words', 'set', 'wound', 'sharpened', 'swords', 'wide', 'targets', 'let', 'shade', 'cover', 'rise', 'like', 'lions', 'slumber', 'in—', 'greatly', 'unknowable', 'numbers', 'free', 'blood', 'must', 'ensue', 'many']\n",
            "Sentence 2: ['waiting', 'mermaid', 'appear', 'saying', 'come', 'little', 'boy', 'drown', 'fear', 'young', 'new', 'explorers', 'spirit', 'doctor', 'livingston', 'talk', 'savage', 'sea', 'direction', 'die', 'together', 'desert', 'escape', 'men', 'prey', 'smile', 'like', 'french', 'resistance', 'laugh', 'guillotine', 'talk', 'savage', 'sea', 'direction', 'disapproved', 'life', 'around', 'created', 'world', 'last', 'request', 'firing', 'squad', 'bullets', 'cannot', 'penetrate', 'sea', 'hide', 'savage', 'sea', 'direction', 'heroes', 'always', 'die', 'battle', 'take', 'violin', 'exiles']\n",
            "Sentence 3: ['waited', 'winterlong', 'seem', 'belong', 'illusion', 'anyway', 'things', 'ever', 'turn', 'wrong', 'love', 'gone', 'easy', 'day', 'bridge', 'waiting', 'follow', 'dreamlight', 'way', 'easy', 'half', 'time', 'passed', 'away', 'shalalala', 'things', 'thought', 'yesterday', 'shalalala', 'come', 'back', 'come', 'back', 'whoaoh', 'instrumental', 'break', 'bridge', 'waiting', 'follow', 'dreamlight', 'way', 'easy', 'half', 'time', 'passed', 'away', 'shalalala', 'things', 'thought', 'yesterday', 'shalalala', 'come', 'back', 'come', 'back', 'whoaoh', 'waited', 'winterlong', 'seem', 'belong', 'waited', 'winterlong', 'seem', 'belong', 'waited', 'winterlong', 'seem', 'belong']\n",
            "Sentence 4: ['world', 'giving', 'run', 'around', 'leaves', 'feeling', 'low', 'let', 'happiness', 'wherever', 'find', 'waiting', 'friend', 'beside', 'lord', 'guide', 'happiness', 'wheverever', 'find', 'wanna', 'find', 'ever', 'felt', 'place', 'smile', 'face', 'keep', 'crying', 'know', 'get', 'touch', 'want', 'much', 'find', 'happiness', 'everybody', 'else', 'winning', 'stuck', 'waiting', 'new', 'beginning', 'happiness', 'wherever', 'find', 'may', 'full', 'life', 'frustrations', 'negotiations', 'happiness', 'wherever', 'find', 'wanna', 'find', 'ever', 'felt', 'place', 'smile', 'face', 'keep', 'crying', 'know', 'get', 'touch', 'want', 'much', 'find', 'happiness', 'got', 'lucky', 'break', 'broke', 'guess', 'got', 'options', 'open', 'happiness', 'wherever', 'find', 'yeah', 'wanna', 'find', 'ever', 'felt', 'place', 'smile', 'face', 'keep', 'crying', 'know', 'get', 'touch', 'want', 'much', 'keep', 'trying', 'people', 'say', 'prayer', 'every', 'day', 'end', 'nextdoor', 'neighboor', 'buying', 'ego', 'trips', 'get', 'kicks', 'find', 'happiness', 'happiness', 'happiness']\n",
            "Sentence 5: ['know', 'say', 'everything', 'life', 'brings', 'today', 'know', 'mean', 'cause', 'even', 'bad', 'change', 'thing', 'cause', 'led', 'one', 'thing', 'different', 'know', 'missing', 'regrets', 'thank', 'god', 'day', 'met', 'somebody', 'must', 'love', 'cause', 'gave', 'somebody', 'must', 'love', 'yes', 'know', 'heart', 'try', 'give', 'love', 'give', 'everything', 'need', 'even', 'know', 'much', 'given', 'somebody', 'must', 'love', 'cause', 'gave', 'would', 'lying', 'said', 'love', 'trying', 'always', 'piece', 'cake', 'mistakes', 'make', 'ups', 'downs', 'always', 'came', 'around', 'still', 'goin', 'nowhere', 'stuck', 'together', 'longer', 'made', 'us', 'stronger', 'regrets', 'thank', 'god', 'day', 'met', 'oh', 'somebody', 'must', 'love', 'cause', 'gave', 'somebody', 'must', 'love', 'yeah', 'yes', 'know', 'heart', 'try', 'give', 'love', 'give', 'everything', 'need', 'even', 'know', 'much', 'given', 'somebody', 'must', 'love', 'cause', 'gave', 'gave', 'bridge', 'oh', 'one', 'else', 'things', 'said', 'one', 'yeah', 'yeah', 'love', 'way', 'lovin', 'oh', 'baby', 'yet', 'feels', 'good', 'babe', 'know', 'love', 'somebody', 'must', 'love', 'woo', 'cause', 'gave', 'somebody', 'must', 'love', 'yeah', 'yes', 'know', 'heart', 'try', 'give', 'love', 'give', 'everything', 'need', 'cause', 'even', 'know', 'much', 'given', 'somebody', 'must', 'love', 'someone', 'yeah', 'cause', 'gave', 'somebody', 'must', 'love', 'somebody', 'must', 'cause', 'gave', 'somebody', 'must', 'love', 'gotta', 'love', 'cause', 'know', 'heart', 'try', 'give', 'love', 'give', 'everything', 'need', 'even', 'know', 'much', 'given', 'somebody', 'must', 'love', 'yeah', 'yeah', 'cause', 'gave', 'somebody', 'must', 'love', 'cause', 'gave', 'cause', 'gave']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming `indices_data` is your list of lists of token indices\n",
        "# And `dataset['labels']` is your list or series of labels\n",
        "\n",
        "class LyricsDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, lyrics_indices, labels):\n",
        "        self.lyrics_indices = lyrics_indices  # List of lists of token indices\n",
        "        self.labels = torch.tensor(labels.tolist()).float()  # Convert labels to tensor\n",
        "        #self.labels = torch.tensor(labels, dtype=torch.float32)  # Convert labels to tensor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.lyrics_indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        lyrics_indices = torch.tensor(self.lyrics_indices[idx], dtype=torch.long)\n",
        "        label = self.labels[idx]\n",
        "        return lyrics_indices, label\n",
        "\n",
        "# Initialize training and validation datasets\n",
        "train_dataset = LyricsDataset(train_indices, train_labels)\n",
        "val_dataset = LyricsDataset(val_indices, val_labels)"
      ],
      "metadata": {
        "id": "BjISLXcMqFI1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    lyrics, labels = zip(*batch)\n",
        "    lyrics = torch.nn.utils.rnn.pad_sequence(lyrics, batch_first=True, padding_value=my_vocab['<pad>'])\n",
        "    labels = torch.stack(labels)\n",
        "    return lyrics, labels\n",
        "\n",
        "# DataLoader for the training set\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "# DataLoader for the validation set\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "MV1FXRYbuxSi"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the GloVe embeddings\n",
        "glove = GloVe(name='6B', dim=300)\n",
        "\n",
        "# Map GloVe embeddings to your vocab\n",
        "def load_pretrained_embeddings(my_vocab, glove):\n",
        "    embeddings = torch.zeros(len(my_vocab), glove.dim)\n",
        "    for word, idx in my_vocab.get_stoi().items():\n",
        "        if word in glove.stoi:\n",
        "            embeddings[idx] = glove.vectors[glove.stoi[word]]\n",
        "        else:\n",
        "            embeddings[idx] = torch.randn(glove.dim)  # Random init for words not in GloVe\n",
        "    return embeddings\n",
        "\n",
        "embedding_matrix = load_pretrained_embeddings(my_vocab, glove)\n",
        "embedding_matrix.requires_grad = False  # Freeze the embeddings if you don't want to fine-tune them"
      ],
      "metadata": {
        "id": "G5CecIAwI9xh"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEDxytAnKEpj",
        "outputId": "8182e1aa-1aff-42a7-ba56-7a96d6d30e43"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([52844, 300])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if CUDA is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Define the model\n",
        "class BOWModel(nn.Module):\n",
        "    def __init__(self, embedding_matrix):\n",
        "        super(BOWModel, self).__init__()\n",
        "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=True)\n",
        "        embedding_dim = embedding_matrix.size(1)  # Get the size of the embeddings\n",
        "\n",
        "        # Define hidden layer sizes\n",
        "        hidden_dim1 = 128  # Size of the first hidden layer\n",
        "        hidden_dim2 = 64   # Size of the second hidden layer\n",
        "        hidden_dim3 = 32   # Size of the third hidden layer\n",
        "        hidden_dim4 = 16   # Size of the second hidden layer\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(embedding_dim, hidden_dim1)\n",
        "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
        "        self.fc3 = nn.Linear(hidden_dim2, hidden_dim3)\n",
        "        self.fc4 = nn.Linear(hidden_dim3, hidden_dim4)\n",
        "        self.fc5 = nn.Linear(hidden_dim4, 1)  # Output layer for binary classification\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.embedding(text)\n",
        "        # Apply an average pooling on the sequence dimension\n",
        "        pooled = embedded.mean(1)  # Assumes (batch_size, seq_length, embedding_dim)\n",
        "\n",
        "        # Pass through the first hidden layer and apply ReLU activation\n",
        "        x = F.relu(self.fc1(pooled))\n",
        "        # Pass through the second hidden layer and apply ReLU activation\n",
        "        x = F.relu(self.fc2(x))\n",
        "\n",
        "        # Pass through the third hidden layer and apply ReLU activation\n",
        "        x = F.relu(self.fc3(x))\n",
        "\n",
        "        # Pass through the fourth hidden layer and apply ReLU activation\n",
        "        x = F.relu(self.fc4(x))\n",
        "\n",
        "        # Output layer\n",
        "        output = self.fc5(x)\n",
        "        return output\n",
        "\n",
        "# Initialize model\n",
        "model = BOWModel(embedding_matrix).to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.BCEWithLogitsLoss().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCmVRuG2d_Hf",
        "outputId": "3c6933f6-6231-4b6a-d00c-ce1324464a64"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training with epochs"
      ],
      "metadata": {
        "id": "6M-HdPOG7qSE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_val_f1_acc = 0  # Initial best is set very low\n",
        "best_model_state = None  # To store the best model's state\n",
        "\n",
        "# Function to train the model\n",
        "def train_model(model, train_loader, val_loader, epochs, initial_epoch=0):\n",
        "\n",
        "    results = []  # List to store results of each epoch\n",
        "    best_val_f1_acc = 0\n",
        "    best_model_state = None\n",
        "\n",
        "    for epoch in range(initial_epoch, initial_epoch + epochs):\n",
        "        model.train()  # Set the model to training mode\n",
        "        total_loss, total_accuracy = 0, 0\n",
        "        all_predictions, all_labels = [], []\n",
        "        for lyrics, labels in train_loader:\n",
        "            lyrics, labels = lyrics.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(lyrics)\n",
        "            loss = criterion(output.squeeze(), labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "            predictions = torch.sigmoid(output).ge(0.5).float()\n",
        "            total_accuracy += (predictions == labels.unsqueeze(1)).float().mean().item()\n",
        "            all_predictions.extend(predictions.cpu().view(-1).tolist())\n",
        "            all_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "        train_loss = total_loss / len(train_loader)\n",
        "        train_accuracy = total_accuracy / len(train_loader)\n",
        "        train_f1 = f1_score(all_labels, [int(p) for p in all_predictions])\n",
        "\n",
        "        # Evaluate the model after each epoch\n",
        "        val_loss, val_acc, val_f1 = evaluate(model, val_loader)\n",
        "\n",
        "        # Save epoch results\n",
        "        results.append({\n",
        "            'Epoch': epoch + 1,\n",
        "            'Train Loss': train_loss,\n",
        "            'Train Accuracy': train_accuracy,\n",
        "            'Train F1': train_f1,\n",
        "            'Validation Loss': val_loss,\n",
        "            'Validation Accuracy': val_acc,\n",
        "            'Validation F1': val_f1\n",
        "        })\n",
        "\n",
        "        # Check if this is the best model based on Validation F1 score\n",
        "        if val_f1+val_acc > best_val_f1_acc:\n",
        "            best_val_f1_acc = val_f1+val_acc\n",
        "            best_model_state = model.state_dict().copy()  # Copy the model state\n",
        "            torch.save(best_model_state, 'best_model.pth')\n",
        "            print(f\"New best model saved at epoch {epoch+1} with Validation F1: {val_f1:.4f} and accuracy: {val_acc:.4f}, sum:{best_val_f1_acc:.4f}\")\n",
        "\n",
        "\n",
        "        # Print epoch results\n",
        "        print(f'Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, Train F1: {train_f1:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}')\n",
        "\n",
        "        # Checkpoint saving\n",
        "        if (epoch + 1) % 100 == 0:  # Save every 100 epochs\n",
        "            torch.save(model.state_dict(), f'model_{epoch+1}_epochs.pth')\n",
        "            torch.save(optimizer.state_dict(), f'optimizer_{epoch+1}_epochs.pth')\n",
        "            print(f\"Checkpoint saved at epoch {epoch+1}\")\n",
        "\n",
        "    # Convert results to DataFrame\n",
        "    results_df = pd.DataFrame(results)\n",
        "    return results_df\n",
        "\n",
        "# Function to evaluate the model\n",
        "def evaluate(model, dataloader, return_predictions=False):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    total_loss, total_accuracy = 0, 0\n",
        "    all_predictions, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for lyrics, labels in dataloader:\n",
        "            lyrics, labels = lyrics.to(device), labels.to(device)\n",
        "            output = model(lyrics)\n",
        "            loss = criterion(output.squeeze(), labels)\n",
        "            total_loss += loss.item()\n",
        "            predictions = torch.sigmoid(output).ge(0.5).float()\n",
        "            total_accuracy += (predictions == labels.unsqueeze(1)).float().mean().item()\n",
        "            all_predictions.extend(predictions.cpu().view(-1).tolist())\n",
        "            all_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "    val_loss = total_loss / len(dataloader)\n",
        "    val_accuracy = total_accuracy / len(dataloader)\n",
        "    val_f1 = f1_score(all_labels, [int(p) for p in all_predictions])\n",
        "\n",
        "    if return_predictions:\n",
        "        return val_loss, val_accuracy, val_f1, all_predictions, all_labels\n",
        "\n",
        "    return val_loss, val_accuracy, val_f1\n"
      ],
      "metadata": {
        "id": "a1Zv0MGOtsfk"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run training and evaluation\n",
        "num_epochs = 100  # Define the number of epochs\n",
        "results_df = train_model(model, train_loader, val_loader, num_epochs)\n",
        "\n",
        "# Save the results to a CSV file\n",
        "results_df.to_csv('training_results_stopWords.csv', index=False)\n",
        "print(\"Results saved to 'training_results_stopWords.csv'\")"
      ],
      "metadata": {
        "id": "I-Y_lq2Nx74C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f071b8e-902a-4d46-8227-5f737b1bfeac"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New best model saved at epoch 1 with Validation F1: 0.6656 and accuracy: 0.4998, sum:1.1654\n",
            "Epoch 1/100 - Train Loss: 0.6937, Train Acc: 0.5028, Train F1: 0.6208, Val Loss: 0.6932, Val Acc: 0.4998, Val F1: 0.6656\n",
            "Epoch 2/100 - Train Loss: 0.6928, Train Acc: 0.5158, Train F1: 0.5399, Val Loss: 0.6913, Val Acc: 0.5388, Val F1: 0.3122\n",
            "New best model saved at epoch 3 with Validation F1: 0.6326 and accuracy: 0.5351, sum:1.1678\n",
            "Epoch 3/100 - Train Loss: 0.6912, Train Acc: 0.5225, Train F1: 0.4545, Val Loss: 0.6863, Val Acc: 0.5351, Val F1: 0.6326\n",
            "Epoch 4/100 - Train Loss: 0.6885, Train Acc: 0.5297, Train F1: 0.3786, Val Loss: 0.6887, Val Acc: 0.5267, Val F1: 0.2185\n",
            "Epoch 5/100 - Train Loss: 0.6860, Train Acc: 0.5381, Train F1: 0.4295, Val Loss: 0.6890, Val Acc: 0.5278, Val F1: 0.2189\n",
            "Epoch 6/100 - Train Loss: 0.6838, Train Acc: 0.5459, Train F1: 0.4540, Val Loss: 0.6844, Val Acc: 0.5386, Val F1: 0.6097\n",
            "Epoch 7/100 - Train Loss: 0.6827, Train Acc: 0.5435, Train F1: 0.4948, Val Loss: 0.6836, Val Acc: 0.5474, Val F1: 0.5199\n",
            "Epoch 8/100 - Train Loss: 0.6826, Train Acc: 0.5476, Train F1: 0.4751, Val Loss: 0.6824, Val Acc: 0.5504, Val F1: 0.5052\n",
            "Epoch 9/100 - Train Loss: 0.6807, Train Acc: 0.5525, Train F1: 0.5147, Val Loss: 0.6803, Val Acc: 0.5581, Val F1: 0.5056\n",
            "Epoch 10/100 - Train Loss: 0.6806, Train Acc: 0.5539, Train F1: 0.5203, Val Loss: 0.6797, Val Acc: 0.5640, Val F1: 0.5554\n",
            "Epoch 11/100 - Train Loss: 0.6800, Train Acc: 0.5580, Train F1: 0.5231, Val Loss: 0.6833, Val Acc: 0.5493, Val F1: 0.2937\n",
            "New best model saved at epoch 12 with Validation F1: 0.6037 and accuracy: 0.5646, sum:1.1683\n",
            "Epoch 12/100 - Train Loss: 0.6778, Train Acc: 0.5591, Train F1: 0.5017, Val Loss: 0.6801, Val Acc: 0.5646, Val F1: 0.6037\n",
            "Epoch 13/100 - Train Loss: 0.6788, Train Acc: 0.5603, Train F1: 0.4819, Val Loss: 0.6835, Val Acc: 0.5521, Val F1: 0.6001\n",
            "New best model saved at epoch 14 with Validation F1: 0.6419 and accuracy: 0.5658, sum:1.2077\n",
            "Epoch 14/100 - Train Loss: 0.6770, Train Acc: 0.5600, Train F1: 0.4704, Val Loss: 0.6868, Val Acc: 0.5658, Val F1: 0.6419\n",
            "Epoch 15/100 - Train Loss: 0.6774, Train Acc: 0.5605, Train F1: 0.4822, Val Loss: 0.6818, Val Acc: 0.5658, Val F1: 0.4903\n",
            "Epoch 16/100 - Train Loss: 0.6776, Train Acc: 0.5645, Train F1: 0.5101, Val Loss: 0.6816, Val Acc: 0.5664, Val F1: 0.5359\n",
            "Epoch 17/100 - Train Loss: 0.6749, Train Acc: 0.5718, Train F1: 0.5511, Val Loss: 0.6849, Val Acc: 0.5776, Val F1: 0.5767\n",
            "Epoch 18/100 - Train Loss: 0.6751, Train Acc: 0.5673, Train F1: 0.5290, Val Loss: 0.6843, Val Acc: 0.5759, Val F1: 0.4778\n",
            "Epoch 19/100 - Train Loss: 0.6750, Train Acc: 0.5700, Train F1: 0.5284, Val Loss: 0.6848, Val Acc: 0.5605, Val F1: 0.5506\n",
            "Epoch 20/100 - Train Loss: 0.6734, Train Acc: 0.5700, Train F1: 0.5366, Val Loss: 0.6927, Val Acc: 0.5858, Val F1: 0.5503\n",
            "New best model saved at epoch 21 with Validation F1: 0.6541 and accuracy: 0.5646, sum:1.2187\n",
            "Epoch 21/100 - Train Loss: 0.6726, Train Acc: 0.5724, Train F1: 0.5365, Val Loss: 0.6961, Val Acc: 0.5646, Val F1: 0.6541\n",
            "Epoch 22/100 - Train Loss: 0.6740, Train Acc: 0.5673, Train F1: 0.5202, Val Loss: 0.6920, Val Acc: 0.5664, Val F1: 0.4955\n",
            "Epoch 23/100 - Train Loss: 0.6680, Train Acc: 0.5730, Train F1: 0.5313, Val Loss: 0.6963, Val Acc: 0.5658, Val F1: 0.6365\n",
            "Epoch 24/100 - Train Loss: 0.6689, Train Acc: 0.5753, Train F1: 0.5454, Val Loss: 0.6869, Val Acc: 0.5699, Val F1: 0.4983\n",
            "Epoch 25/100 - Train Loss: 0.6693, Train Acc: 0.5730, Train F1: 0.5334, Val Loss: 0.6967, Val Acc: 0.5676, Val F1: 0.5948\n",
            "Epoch 26/100 - Train Loss: 0.6686, Train Acc: 0.5801, Train F1: 0.5478, Val Loss: 0.6886, Val Acc: 0.5646, Val F1: 0.5502\n",
            "Epoch 27/100 - Train Loss: 0.6676, Train Acc: 0.5690, Train F1: 0.5180, Val Loss: 0.6968, Val Acc: 0.5409, Val F1: 0.2891\n",
            "Epoch 28/100 - Train Loss: 0.6666, Train Acc: 0.5794, Train F1: 0.5249, Val Loss: 0.6994, Val Acc: 0.5741, Val F1: 0.5122\n",
            "Epoch 29/100 - Train Loss: 0.6665, Train Acc: 0.5761, Train F1: 0.5350, Val Loss: 0.7064, Val Acc: 0.5764, Val F1: 0.5062\n",
            "Epoch 30/100 - Train Loss: 0.6634, Train Acc: 0.5835, Train F1: 0.5547, Val Loss: 0.7044, Val Acc: 0.5351, Val F1: 0.6669\n",
            "Epoch 31/100 - Train Loss: 0.6631, Train Acc: 0.5810, Train F1: 0.5731, Val Loss: 0.6975, Val Acc: 0.5723, Val F1: 0.5158\n",
            "Epoch 32/100 - Train Loss: 0.6637, Train Acc: 0.5806, Train F1: 0.5499, Val Loss: 0.7060, Val Acc: 0.5829, Val F1: 0.4686\n",
            "Epoch 33/100 - Train Loss: 0.6635, Train Acc: 0.5814, Train F1: 0.5334, Val Loss: 0.7155, Val Acc: 0.5782, Val F1: 0.6363\n",
            "Epoch 34/100 - Train Loss: 0.6613, Train Acc: 0.5864, Train F1: 0.5665, Val Loss: 0.7137, Val Acc: 0.5829, Val F1: 0.5531\n",
            "Epoch 35/100 - Train Loss: 0.6583, Train Acc: 0.5897, Train F1: 0.5505, Val Loss: 0.7590, Val Acc: 0.5522, Val F1: 0.3661\n",
            "Epoch 36/100 - Train Loss: 0.6617, Train Acc: 0.5879, Train F1: 0.5606, Val Loss: 0.7096, Val Acc: 0.5752, Val F1: 0.5227\n",
            "New best model saved at epoch 37 with Validation F1: 0.6467 and accuracy: 0.5794, sum:1.2261\n",
            "Epoch 37/100 - Train Loss: 0.6587, Train Acc: 0.5856, Train F1: 0.5697, Val Loss: 0.7367, Val Acc: 0.5794, Val F1: 0.6467\n",
            "Epoch 38/100 - Train Loss: 0.6568, Train Acc: 0.5890, Train F1: 0.5726, Val Loss: 0.7431, Val Acc: 0.5692, Val F1: 0.5021\n",
            "Epoch 39/100 - Train Loss: 0.6599, Train Acc: 0.5831, Train F1: 0.5380, Val Loss: 0.7144, Val Acc: 0.5829, Val F1: 0.4946\n",
            "Epoch 40/100 - Train Loss: 0.6579, Train Acc: 0.5932, Train F1: 0.5627, Val Loss: 0.7163, Val Acc: 0.5847, Val F1: 0.6261\n",
            "Epoch 41/100 - Train Loss: 0.6582, Train Acc: 0.5835, Train F1: 0.5367, Val Loss: 0.7207, Val Acc: 0.5441, Val F1: 0.6678\n",
            "Epoch 42/100 - Train Loss: 0.6596, Train Acc: 0.5839, Train F1: 0.5644, Val Loss: 0.7087, Val Acc: 0.5710, Val F1: 0.4432\n",
            "Epoch 43/100 - Train Loss: 0.6580, Train Acc: 0.5839, Train F1: 0.5494, Val Loss: 0.7168, Val Acc: 0.5882, Val F1: 0.6305\n",
            "Epoch 44/100 - Train Loss: 0.6550, Train Acc: 0.5902, Train F1: 0.5467, Val Loss: 0.7255, Val Acc: 0.5545, Val F1: 0.3593\n",
            "Epoch 45/100 - Train Loss: 0.6529, Train Acc: 0.5914, Train F1: 0.5574, Val Loss: 0.7230, Val Acc: 0.5810, Val F1: 0.5789\n",
            "Epoch 46/100 - Train Loss: 0.6571, Train Acc: 0.5845, Train F1: 0.5603, Val Loss: 0.7107, Val Acc: 0.5776, Val F1: 0.5062\n",
            "Epoch 47/100 - Train Loss: 0.6525, Train Acc: 0.5939, Train F1: 0.5693, Val Loss: 0.7119, Val Acc: 0.5723, Val F1: 0.4407\n",
            "Epoch 48/100 - Train Loss: 0.6528, Train Acc: 0.5871, Train F1: 0.5576, Val Loss: 0.7216, Val Acc: 0.5788, Val F1: 0.5137\n",
            "Epoch 49/100 - Train Loss: 0.6527, Train Acc: 0.5914, Train F1: 0.5705, Val Loss: 0.7452, Val Acc: 0.5520, Val F1: 0.3554\n",
            "Epoch 50/100 - Train Loss: 0.6475, Train Acc: 0.5987, Train F1: 0.5794, Val Loss: 0.7468, Val Acc: 0.5663, Val F1: 0.6414\n",
            "Epoch 51/100 - Train Loss: 0.6487, Train Acc: 0.5945, Train F1: 0.5731, Val Loss: 0.7481, Val Acc: 0.5574, Val F1: 0.3641\n",
            "Epoch 52/100 - Train Loss: 0.6455, Train Acc: 0.6020, Train F1: 0.5813, Val Loss: 0.7510, Val Acc: 0.5410, Val F1: 0.6620\n",
            "Epoch 53/100 - Train Loss: 0.6494, Train Acc: 0.5977, Train F1: 0.5915, Val Loss: 0.7423, Val Acc: 0.5757, Val F1: 0.4761\n",
            "Epoch 54/100 - Train Loss: 0.6463, Train Acc: 0.5993, Train F1: 0.5589, Val Loss: 0.7623, Val Acc: 0.5848, Val F1: 0.6034\n",
            "Epoch 55/100 - Train Loss: 0.6460, Train Acc: 0.6020, Train F1: 0.5838, Val Loss: 0.7384, Val Acc: 0.5657, Val F1: 0.4332\n",
            "New best model saved at epoch 56 with Validation F1: 0.6478 and accuracy: 0.5935, sum:1.2413\n",
            "Epoch 56/100 - Train Loss: 0.6462, Train Acc: 0.5978, Train F1: 0.5911, Val Loss: 0.7550, Val Acc: 0.5935, Val F1: 0.6478\n",
            "Epoch 57/100 - Train Loss: 0.6454, Train Acc: 0.5994, Train F1: 0.5919, Val Loss: 0.7618, Val Acc: 0.5722, Val F1: 0.4990\n",
            "Epoch 58/100 - Train Loss: 0.6449, Train Acc: 0.6048, Train F1: 0.5857, Val Loss: 1.0874, Val Acc: 0.5362, Val F1: 0.3048\n",
            "Epoch 59/100 - Train Loss: 0.6479, Train Acc: 0.6021, Train F1: 0.5832, Val Loss: 0.7494, Val Acc: 0.5858, Val F1: 0.6030\n",
            "Epoch 60/100 - Train Loss: 0.6433, Train Acc: 0.6003, Train F1: 0.5804, Val Loss: 0.7329, Val Acc: 0.5764, Val F1: 0.5376\n",
            "New best model saved at epoch 61 with Validation F1: 0.6660 and accuracy: 0.5877, sum:1.2538\n",
            "Epoch 61/100 - Train Loss: 0.6437, Train Acc: 0.6012, Train F1: 0.5800, Val Loss: 0.7509, Val Acc: 0.5877, Val F1: 0.6660\n",
            "Epoch 62/100 - Train Loss: 0.6406, Train Acc: 0.6070, Train F1: 0.5903, Val Loss: 0.7712, Val Acc: 0.5611, Val F1: 0.4003\n",
            "Epoch 63/100 - Train Loss: 0.6431, Train Acc: 0.6033, Train F1: 0.5826, Val Loss: 0.7856, Val Acc: 0.5830, Val F1: 0.6519\n",
            "Epoch 64/100 - Train Loss: 0.6516, Train Acc: 0.5922, Train F1: 0.5615, Val Loss: 0.7500, Val Acc: 0.5517, Val F1: 0.6702\n",
            "Epoch 65/100 - Train Loss: 0.6432, Train Acc: 0.6031, Train F1: 0.5942, Val Loss: 0.7556, Val Acc: 0.5994, Val F1: 0.6392\n",
            "Epoch 66/100 - Train Loss: 0.6432, Train Acc: 0.6034, Train F1: 0.5955, Val Loss: 0.7957, Val Acc: 0.5281, Val F1: 0.6667\n",
            "Epoch 67/100 - Train Loss: 0.6423, Train Acc: 0.6042, Train F1: 0.5962, Val Loss: 0.7426, Val Acc: 0.5857, Val F1: 0.6036\n",
            "Epoch 68/100 - Train Loss: 0.6399, Train Acc: 0.6065, Train F1: 0.6010, Val Loss: 0.7625, Val Acc: 0.5835, Val F1: 0.6185\n",
            "Epoch 69/100 - Train Loss: 0.6408, Train Acc: 0.6073, Train F1: 0.5860, Val Loss: 0.7862, Val Acc: 0.5795, Val F1: 0.5520\n",
            "Epoch 70/100 - Train Loss: 0.6389, Train Acc: 0.6097, Train F1: 0.5954, Val Loss: 0.7413, Val Acc: 0.5901, Val F1: 0.6228\n",
            "Epoch 71/100 - Train Loss: 0.6393, Train Acc: 0.6063, Train F1: 0.5980, Val Loss: 0.7384, Val Acc: 0.5882, Val F1: 0.5879\n",
            "Epoch 72/100 - Train Loss: 0.6396, Train Acc: 0.6099, Train F1: 0.5920, Val Loss: 0.8003, Val Acc: 0.5133, Val F1: 0.6592\n",
            "Epoch 73/100 - Train Loss: 0.6385, Train Acc: 0.6069, Train F1: 0.5959, Val Loss: 0.8266, Val Acc: 0.5178, Val F1: 0.1918\n",
            "Epoch 74/100 - Train Loss: 0.6387, Train Acc: 0.6113, Train F1: 0.5972, Val Loss: 0.7838, Val Acc: 0.5900, Val F1: 0.6505\n",
            "Epoch 75/100 - Train Loss: 0.6358, Train Acc: 0.6133, Train F1: 0.6054, Val Loss: 0.8187, Val Acc: 0.5559, Val F1: 0.6753\n",
            "Epoch 76/100 - Train Loss: 0.6333, Train Acc: 0.6099, Train F1: 0.5958, Val Loss: 0.7806, Val Acc: 0.5923, Val F1: 0.6292\n",
            "Epoch 77/100 - Train Loss: 0.6370, Train Acc: 0.6074, Train F1: 0.5903, Val Loss: 0.7948, Val Acc: 0.5471, Val F1: 0.6687\n",
            "Epoch 78/100 - Train Loss: 0.6346, Train Acc: 0.6088, Train F1: 0.5959, Val Loss: 0.8228, Val Acc: 0.5498, Val F1: 0.3830\n",
            "Epoch 79/100 - Train Loss: 0.6358, Train Acc: 0.6096, Train F1: 0.5886, Val Loss: 0.7931, Val Acc: 0.5783, Val F1: 0.5409\n",
            "Epoch 80/100 - Train Loss: 0.6363, Train Acc: 0.6037, Train F1: 0.5813, Val Loss: 0.8369, Val Acc: 0.5695, Val F1: 0.6676\n",
            "Epoch 81/100 - Train Loss: 0.6341, Train Acc: 0.6126, Train F1: 0.5969, Val Loss: 0.7968, Val Acc: 0.5752, Val F1: 0.5246\n",
            "Epoch 82/100 - Train Loss: 0.6341, Train Acc: 0.6150, Train F1: 0.6032, Val Loss: 0.8387, Val Acc: 0.5870, Val F1: 0.6503\n",
            "Epoch 83/100 - Train Loss: 0.6353, Train Acc: 0.6121, Train F1: 0.5991, Val Loss: 0.8274, Val Acc: 0.5864, Val F1: 0.6210\n",
            "Epoch 84/100 - Train Loss: 0.6310, Train Acc: 0.6162, Train F1: 0.6047, Val Loss: 0.7747, Val Acc: 0.5894, Val F1: 0.6086\n",
            "Epoch 85/100 - Train Loss: 0.6318, Train Acc: 0.6122, Train F1: 0.6030, Val Loss: 0.8319, Val Acc: 0.5517, Val F1: 0.4000\n",
            "Epoch 86/100 - Train Loss: 0.6313, Train Acc: 0.6137, Train F1: 0.5942, Val Loss: 0.7821, Val Acc: 0.5781, Val F1: 0.5258\n",
            "Epoch 87/100 - Train Loss: 0.6329, Train Acc: 0.6096, Train F1: 0.5887, Val Loss: 0.7894, Val Acc: 0.5710, Val F1: 0.5038\n",
            "Epoch 88/100 - Train Loss: 0.6315, Train Acc: 0.6130, Train F1: 0.5997, Val Loss: 0.8175, Val Acc: 0.5857, Val F1: 0.6180\n",
            "Epoch 89/100 - Train Loss: 0.6351, Train Acc: 0.6072, Train F1: 0.5937, Val Loss: 0.7725, Val Acc: 0.5800, Val F1: 0.6590\n",
            "Epoch 90/100 - Train Loss: 0.6327, Train Acc: 0.6178, Train F1: 0.6024, Val Loss: 0.7822, Val Acc: 0.5923, Val F1: 0.6344\n",
            "Epoch 91/100 - Train Loss: 0.6331, Train Acc: 0.6180, Train F1: 0.6088, Val Loss: 0.7846, Val Acc: 0.5735, Val F1: 0.5223\n",
            "Epoch 92/100 - Train Loss: 0.6311, Train Acc: 0.6101, Train F1: 0.6037, Val Loss: 0.8139, Val Acc: 0.5711, Val F1: 0.5038\n",
            "Epoch 93/100 - Train Loss: 0.6291, Train Acc: 0.6133, Train F1: 0.6205, Val Loss: 0.7699, Val Acc: 0.5752, Val F1: 0.5089\n",
            "Epoch 94/100 - Train Loss: 0.6287, Train Acc: 0.6166, Train F1: 0.6032, Val Loss: 0.8001, Val Acc: 0.5617, Val F1: 0.4363\n",
            "Epoch 95/100 - Train Loss: 0.6301, Train Acc: 0.6186, Train F1: 0.6103, Val Loss: 0.8053, Val Acc: 0.5811, Val F1: 0.5933\n",
            "Epoch 96/100 - Train Loss: 0.6264, Train Acc: 0.6176, Train F1: 0.6085, Val Loss: 0.8283, Val Acc: 0.5830, Val F1: 0.6573\n",
            "Epoch 97/100 - Train Loss: 0.6275, Train Acc: 0.6232, Train F1: 0.6192, Val Loss: 0.8290, Val Acc: 0.5795, Val F1: 0.5883\n",
            "Epoch 98/100 - Train Loss: 0.6288, Train Acc: 0.6149, Train F1: 0.5971, Val Loss: 0.8970, Val Acc: 0.5848, Val F1: 0.6426\n",
            "Epoch 99/100 - Train Loss: 0.6327, Train Acc: 0.6096, Train F1: 0.5889, Val Loss: 0.7904, Val Acc: 0.5676, Val F1: 0.4706\n",
            "Epoch 100/100 - Train Loss: 0.6276, Train Acc: 0.6173, Train F1: 0.6053, Val Loss: 0.8315, Val Acc: 0.5676, Val F1: 0.4439\n",
            "Checkpoint saved at epoch 100\n",
            "Results saved to 'training_results_stopWords.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load model and optimizer state\n",
        "model.load_state_dict(torch.load('model_200_epochs.pth'))\n",
        "optimizer.load_state_dict(torch.load('optimizer_200_epochs.pth'))\n",
        "model.to(device)  # Make sure to reassign the model to the device\n",
        "\n",
        "# Now train for an additional X epochs\n",
        "further_train = 20\n",
        "train_model(model, train_loader, val_loader, further_train, initial_epoch=240)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "J1XqF6389Hm4",
        "outputId": "bd95c4f7-739c-4f5c-eccc-764f637515b9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New best model saved at epoch 241 with Validation F1: 0.5859 and accuracy: 0.5851, sum:1.1710\n",
            "Epoch 241/20 - Train Loss: 0.6025, Train Acc: 0.6339, Train F1: 0.6216, Val Loss: 1.0746, Val Acc: 0.5851, Val F1: 0.5859\n",
            "New best model saved at epoch 242 with Validation F1: 0.6613 and accuracy: 0.5794, sum:1.2406\n",
            "Epoch 242/20 - Train Loss: 0.5993, Train Acc: 0.6344, Train F1: 0.6274, Val Loss: 1.2967, Val Acc: 0.5794, Val F1: 0.6613\n",
            "Epoch 243/20 - Train Loss: 0.5960, Train Acc: 0.6388, Train F1: 0.6333, Val Loss: 1.2086, Val Acc: 0.5739, Val F1: 0.5451\n",
            "Epoch 244/20 - Train Loss: 0.6017, Train Acc: 0.6414, Train F1: 0.6333, Val Loss: 1.0630, Val Acc: 0.5853, Val F1: 0.6110\n",
            "Epoch 245/20 - Train Loss: 0.5962, Train Acc: 0.6426, Train F1: 0.6379, Val Loss: 1.1492, Val Acc: 0.5763, Val F1: 0.5807\n",
            "Epoch 246/20 - Train Loss: 0.5970, Train Acc: 0.6451, Train F1: 0.6389, Val Loss: 1.1973, Val Acc: 0.5675, Val F1: 0.5771\n",
            "Epoch 247/20 - Train Loss: 0.5986, Train Acc: 0.6390, Train F1: 0.6363, Val Loss: 1.1322, Val Acc: 0.5693, Val F1: 0.5121\n",
            "Epoch 248/20 - Train Loss: 0.5985, Train Acc: 0.6388, Train F1: 0.6306, Val Loss: 1.2250, Val Acc: 0.5699, Val F1: 0.4689\n",
            "Epoch 249/20 - Train Loss: 0.6009, Train Acc: 0.6391, Train F1: 0.6330, Val Loss: 1.1692, Val Acc: 0.5710, Val F1: 0.6529\n",
            "Epoch 250/20 - Train Loss: 0.5974, Train Acc: 0.6464, Train F1: 0.6386, Val Loss: 1.0958, Val Acc: 0.5763, Val F1: 0.5389\n",
            "Epoch 251/20 - Train Loss: 0.5988, Train Acc: 0.6404, Train F1: 0.6339, Val Loss: 1.1500, Val Acc: 0.5757, Val F1: 0.5909\n",
            "Epoch 252/20 - Train Loss: 0.5970, Train Acc: 0.6436, Train F1: 0.6367, Val Loss: 1.2435, Val Acc: 0.5739, Val F1: 0.5369\n",
            "Epoch 253/20 - Train Loss: 0.5999, Train Acc: 0.6392, Train F1: 0.6326, Val Loss: 1.0861, Val Acc: 0.5686, Val F1: 0.5477\n",
            "Epoch 254/20 - Train Loss: 0.5977, Train Acc: 0.6397, Train F1: 0.6343, Val Loss: 1.0576, Val Acc: 0.5847, Val F1: 0.5472\n",
            "Epoch 255/20 - Train Loss: 0.6032, Train Acc: 0.6373, Train F1: 0.6310, Val Loss: 1.0541, Val Acc: 0.5745, Val F1: 0.5821\n",
            "Epoch 256/20 - Train Loss: 0.5957, Train Acc: 0.6402, Train F1: 0.6343, Val Loss: 1.1260, Val Acc: 0.5829, Val F1: 0.6455\n",
            "Epoch 257/20 - Train Loss: 0.5933, Train Acc: 0.6464, Train F1: 0.6435, Val Loss: 1.1400, Val Acc: 0.5775, Val F1: 0.6570\n",
            "Epoch 258/20 - Train Loss: 0.5965, Train Acc: 0.6454, Train F1: 0.6422, Val Loss: 1.2412, Val Acc: 0.5685, Val F1: 0.6038\n",
            "Epoch 259/20 - Train Loss: 0.5941, Train Acc: 0.6433, Train F1: 0.6405, Val Loss: 1.2757, Val Acc: 0.5520, Val F1: 0.6673\n",
            "Epoch 260/20 - Train Loss: 0.5971, Train Acc: 0.6378, Train F1: 0.6336, Val Loss: 1.2327, Val Acc: 0.5798, Val F1: 0.5827\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Epoch  Train Loss  Train Accuracy  Train F1  Validation Loss  \\\n",
              "0     241    0.602493        0.633944  0.621574         1.074638   \n",
              "1     242    0.599311        0.634406  0.627411         1.296731   \n",
              "2     243    0.595978        0.638786  0.633313         1.208617   \n",
              "3     244    0.601738        0.641445  0.633304         1.062955   \n",
              "4     245    0.596218        0.642552  0.637900         1.149193   \n",
              "5     246    0.596997        0.645101  0.638926         1.197326   \n",
              "6     247    0.598638        0.638969  0.636273         1.132209   \n",
              "7     248    0.598492        0.638779  0.630621         1.225023   \n",
              "8     249    0.600932        0.639087  0.632998         1.169165   \n",
              "9     250    0.597433        0.646405  0.638576         1.095822   \n",
              "10    251    0.598832        0.640391  0.633904         1.149991   \n",
              "11    252    0.597045        0.643621  0.636681         1.243527   \n",
              "12    253    0.599862        0.639189  0.632554         1.086076   \n",
              "13    254    0.597678        0.639731  0.634264         1.057551   \n",
              "14    255    0.603180        0.637307  0.630964         1.054120   \n",
              "15    256    0.595652        0.640237  0.634297         1.126046   \n",
              "16    257    0.593290        0.646390  0.643484         1.139991   \n",
              "17    258    0.596523        0.645416  0.642173         1.241151   \n",
              "18    259    0.594107        0.643306  0.640521         1.275722   \n",
              "19    260    0.597058        0.637834  0.633616         1.232730   \n",
              "\n",
              "    Validation Accuracy  Validation F1  \n",
              "0              0.585142       0.585859  \n",
              "1              0.579363       0.661252  \n",
              "2              0.573939       0.545108  \n",
              "3              0.585259       0.611018  \n",
              "4              0.576297       0.580683  \n",
              "5              0.567453       0.577080  \n",
              "6              0.569340       0.512097  \n",
              "7              0.569929       0.468864  \n",
              "8              0.570991       0.652865  \n",
              "9              0.576297       0.538860  \n",
              "10             0.575708       0.590935  \n",
              "11             0.573939       0.536869  \n",
              "12             0.568632       0.547723  \n",
              "13             0.584670       0.547219  \n",
              "14             0.574528       0.582116  \n",
              "15             0.582901       0.645487  \n",
              "16             0.577476       0.657005  \n",
              "17             0.568514       0.603836  \n",
              "18             0.552005       0.667257  \n",
              "19             0.579835       0.582742  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-69ec1a59-3ca5-4f3f-8865-92c631dec012\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Epoch</th>\n",
              "      <th>Train Loss</th>\n",
              "      <th>Train Accuracy</th>\n",
              "      <th>Train F1</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Validation Accuracy</th>\n",
              "      <th>Validation F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>241</td>\n",
              "      <td>0.602493</td>\n",
              "      <td>0.633944</td>\n",
              "      <td>0.621574</td>\n",
              "      <td>1.074638</td>\n",
              "      <td>0.585142</td>\n",
              "      <td>0.585859</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>242</td>\n",
              "      <td>0.599311</td>\n",
              "      <td>0.634406</td>\n",
              "      <td>0.627411</td>\n",
              "      <td>1.296731</td>\n",
              "      <td>0.579363</td>\n",
              "      <td>0.661252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>243</td>\n",
              "      <td>0.595978</td>\n",
              "      <td>0.638786</td>\n",
              "      <td>0.633313</td>\n",
              "      <td>1.208617</td>\n",
              "      <td>0.573939</td>\n",
              "      <td>0.545108</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>244</td>\n",
              "      <td>0.601738</td>\n",
              "      <td>0.641445</td>\n",
              "      <td>0.633304</td>\n",
              "      <td>1.062955</td>\n",
              "      <td>0.585259</td>\n",
              "      <td>0.611018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>245</td>\n",
              "      <td>0.596218</td>\n",
              "      <td>0.642552</td>\n",
              "      <td>0.637900</td>\n",
              "      <td>1.149193</td>\n",
              "      <td>0.576297</td>\n",
              "      <td>0.580683</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>246</td>\n",
              "      <td>0.596997</td>\n",
              "      <td>0.645101</td>\n",
              "      <td>0.638926</td>\n",
              "      <td>1.197326</td>\n",
              "      <td>0.567453</td>\n",
              "      <td>0.577080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>247</td>\n",
              "      <td>0.598638</td>\n",
              "      <td>0.638969</td>\n",
              "      <td>0.636273</td>\n",
              "      <td>1.132209</td>\n",
              "      <td>0.569340</td>\n",
              "      <td>0.512097</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>248</td>\n",
              "      <td>0.598492</td>\n",
              "      <td>0.638779</td>\n",
              "      <td>0.630621</td>\n",
              "      <td>1.225023</td>\n",
              "      <td>0.569929</td>\n",
              "      <td>0.468864</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>249</td>\n",
              "      <td>0.600932</td>\n",
              "      <td>0.639087</td>\n",
              "      <td>0.632998</td>\n",
              "      <td>1.169165</td>\n",
              "      <td>0.570991</td>\n",
              "      <td>0.652865</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>250</td>\n",
              "      <td>0.597433</td>\n",
              "      <td>0.646405</td>\n",
              "      <td>0.638576</td>\n",
              "      <td>1.095822</td>\n",
              "      <td>0.576297</td>\n",
              "      <td>0.538860</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>251</td>\n",
              "      <td>0.598832</td>\n",
              "      <td>0.640391</td>\n",
              "      <td>0.633904</td>\n",
              "      <td>1.149991</td>\n",
              "      <td>0.575708</td>\n",
              "      <td>0.590935</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>252</td>\n",
              "      <td>0.597045</td>\n",
              "      <td>0.643621</td>\n",
              "      <td>0.636681</td>\n",
              "      <td>1.243527</td>\n",
              "      <td>0.573939</td>\n",
              "      <td>0.536869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>253</td>\n",
              "      <td>0.599862</td>\n",
              "      <td>0.639189</td>\n",
              "      <td>0.632554</td>\n",
              "      <td>1.086076</td>\n",
              "      <td>0.568632</td>\n",
              "      <td>0.547723</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>254</td>\n",
              "      <td>0.597678</td>\n",
              "      <td>0.639731</td>\n",
              "      <td>0.634264</td>\n",
              "      <td>1.057551</td>\n",
              "      <td>0.584670</td>\n",
              "      <td>0.547219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>255</td>\n",
              "      <td>0.603180</td>\n",
              "      <td>0.637307</td>\n",
              "      <td>0.630964</td>\n",
              "      <td>1.054120</td>\n",
              "      <td>0.574528</td>\n",
              "      <td>0.582116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>256</td>\n",
              "      <td>0.595652</td>\n",
              "      <td>0.640237</td>\n",
              "      <td>0.634297</td>\n",
              "      <td>1.126046</td>\n",
              "      <td>0.582901</td>\n",
              "      <td>0.645487</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>257</td>\n",
              "      <td>0.593290</td>\n",
              "      <td>0.646390</td>\n",
              "      <td>0.643484</td>\n",
              "      <td>1.139991</td>\n",
              "      <td>0.577476</td>\n",
              "      <td>0.657005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>258</td>\n",
              "      <td>0.596523</td>\n",
              "      <td>0.645416</td>\n",
              "      <td>0.642173</td>\n",
              "      <td>1.241151</td>\n",
              "      <td>0.568514</td>\n",
              "      <td>0.603836</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>259</td>\n",
              "      <td>0.594107</td>\n",
              "      <td>0.643306</td>\n",
              "      <td>0.640521</td>\n",
              "      <td>1.275722</td>\n",
              "      <td>0.552005</td>\n",
              "      <td>0.667257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>260</td>\n",
              "      <td>0.597058</td>\n",
              "      <td>0.637834</td>\n",
              "      <td>0.633616</td>\n",
              "      <td>1.232730</td>\n",
              "      <td>0.579835</td>\n",
              "      <td>0.582742</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-69ec1a59-3ca5-4f3f-8865-92c631dec012')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-69ec1a59-3ca5-4f3f-8865-92c631dec012 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-69ec1a59-3ca5-4f3f-8865-92c631dec012');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-4cd7ce41-93cc-47da-a184-be45941e9716\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4cd7ce41-93cc-47da-a184-be45941e9716')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-4cd7ce41-93cc-47da-a184-be45941e9716 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"train_model(model, train_loader, val_loader, further_train, initial_epoch=240)\",\n  \"rows\": 20,\n  \"fields\": [\n    {\n      \"column\": \"Epoch\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5,\n        \"min\": 241,\n        \"max\": 260,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          241,\n          258,\n          256\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Train Loss\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.002631222550240936,\n        \"min\": 0.593289809733755,\n        \"max\": 0.6031799067110452,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          0.6024928924780858,\n          0.5965227369329095,\n          0.5956517915999839\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Train Accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.003607307513396631,\n        \"min\": 0.6339442686566824,\n        \"max\": 0.6464047116946571,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          0.6339442686566824,\n          0.6454157876440242,\n          0.6402367557465779\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Train F1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.005057243889744509,\n        \"min\": 0.6215737078958136,\n        \"max\": 0.6434840425531916,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          0.6215737078958136,\n          0.6421725239616615,\n          0.6342968278452149\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Validation Loss\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07592486042725011,\n        \"min\": 1.05412010764176,\n        \"max\": 1.2967306434545878,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          1.074638178888357,\n          1.2411511512297504,\n          1.1260461233696848\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Validation Accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.007807158813555401,\n        \"min\": 0.5520047170092475,\n        \"max\": 0.5852594341871873,\n        \"num_unique_values\": 18,\n        \"samples\": [\n          0.5851415094901931,\n          0.579363207772093,\n          0.5709905660939667\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Validation F1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.053727824610364974,\n        \"min\": 0.46886446886446886,\n        \"max\": 0.667257421355782,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          0.585858585858586,\n          0.6038356164383561,\n          0.6454866364094806\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the results to a CSV file\n",
        "results_df.to_csv('training_results_stopWords_240epochs.csv', index=False)\n",
        "print(\"Results saved to 'training_results_stopWords_240epochs.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fmAEFFLC3PV",
        "outputId": "b4db7866-e081-429f-9f57-bab48a0faaa1"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to 'training_results_stopWords_240epochs.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume you're somewhere else in your code and need to load the best model\n",
        "model.load_state_dict(torch.load('best_model.pth'))\n",
        "print(\"Loaded the best model from disk.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOjg3zISFFkj",
        "outputId": "0e10b030-fd91-46fb-a694-6e032ce682be"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded the best model from disk.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Compute the confusion matrix for the last epoch's validation data\n",
        "_, val_accuracy, val_f1, predictions, labels = evaluate(model, val_loader, return_predictions=True)\n",
        "cm = confusion_matrix(labels, predictions)\n",
        "\n",
        "# Plotting the confusion matrix\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap='Blues', cbar=False)\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('True labels')\n",
        "ax.set_title('Confusion Matrix: DAN with Glove (300 D) - stop words removed')\n",
        "plt.show()\n",
        "\n",
        "# Save the results to a CSV file\n",
        "results_df.to_csv('training_results.csv', index=False)\n",
        "print(\"Results saved to 'training_results.csv'\")\n",
        "\n",
        "print(\"Accuracy:\", val_accuracy)\n",
        "print(\"F1 Score:\", val_f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 773
        },
        "id": "bOW1WQVdvxvq",
        "outputId": "923f8d9e-b1de-43ed-9637-248ace350681"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to 'training_results.csv'\n",
            "Accuracy: 0.5877358491690654\n",
            "F1 Score: 0.6660277910876857\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Cxex0s0Yh_hX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}